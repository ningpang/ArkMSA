[training]
batch_size = 16
gradient_accumulation_steps = 1
drop_out = 0.5
num_workers = 4
epochs = 10
device = cuda:0
seed = 2022
max_grad_norm = 10
lr = 1e-5
use_strictloss = False
weight_decay = 0.0
p_mask = 0.00
use_marker = False
mlm_loss_weight = 0.0

[Encoder]
encoder_type = roberta
roberta_path = roberta-large
bert_path = /home/pangning/Projects/RevisedKIP/pretrained_bert
max_length = 400
reserve_length = 120
vocab_size = 50265
;vocab_size = 30522
encoder_output_size = 768
model_type = cls


[data]
dataset_name = single
task = MVSA
root_dir = ./data/mvsas
init_new_token_by_cls = False
template_id = 6

[extra]
p_mask = 0
rewrite_p = 0.0
average = weighted
