[training]
batch_size = 4
gradient_accumulation_steps = 1
drop_out = 0.5
num_workers = 4
epochs = 10
device = cuda:0
seed = 2022
max_grad_norm = 10
lr = 1e-5
use_strictloss = False
weight_decay = 0.0


[Encoder]
encoder_type = roberta
roberta_path = roberta-large
bert_path = /home/pangning/Projects/RevisedKIP/pretrained_bert
max_length = 400
reserve_length = 120
vocab_size = 50265
;vocab_size = 30522
encoder_output_size = 768
model_type = mlmp


[data]
dataset_name = MVSA_single
task = TW17K
root_dir = ./data/Twitter17
init_new_token_by_cls = False
template_id = 6

[extra]
p_mask = 0.3
rewrite_p = 0.0
average = macro
alpha = 0.5
