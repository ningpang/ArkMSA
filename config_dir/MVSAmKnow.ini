[training]
batch_size = 4
gradient_accumulation_steps = 1
drop_out = 0.5
num_workers = 4
epochs = 10
device = cuda:0
seed = 2022
max_grad_norm = 10
lr = 5e-6
use_strictloss = False
weight_decay = 0.0
p_mask = 0.00
use_marker = False
mlm_loss_weight = 0.0

[Encoder]
encoder_type = bert
roberta_path = roberta-large
bert_path = /home/pangning/Projects/RevisedKIP/pretrained_bert
max_length = 500
reserve_length = 180
vocab_size = 50265
;vocab_size = 30522
encoder_output_size = 768
model_type = mlm


[data]
dataset_name = multiple
task = MVSAK
root_dir = ./data/mvsam
init_new_token_by_cls = False
template_id = 6

[extra]
p_mask = 0.15
rewrite_p = 0.0
average = macro
alpha = 0.1